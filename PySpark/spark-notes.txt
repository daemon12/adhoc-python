Tutorial: Introduction to Apache Spark:
=======================================
https://www.dezyre.com/apache-spark-tutorial/tutorial-introduction-to-apache-spark

Reason behind its invention
  -Exploding  Data-> Hadoop-> But Hadoop has several limitation 
      --Hadoop is strict MapReduce
      --Limited API expansion
      --Not meant for iterative processing (ML)
      --Pipelining of tasks not easy
  -Data Manipulation speed-> analysis might not be relevant after time is gone
  
What is Spark??
  an open source data processing framework 
            for performing Big data analytics 
                  on distributed computing cluster
  started by "Matei Zaharia" at UC Berkeley's AMPLab in 2009 (academic project)
      -idea was to build a cluster management tool: result --> Mesos
      -after Mesos built > cluster computing framework on top of it --> Spark
  2013 -> passed on to the Apache Software Foundation

Spark Features:
    faster than MaReduce, low latency due to reduced disk IO
    capability of in memory computation and operations
        Unlike Hadoop, intermediate results kept in memory than writing to disk
        When data crosses threshold of the memory, it is spilled to the disk
    lazy evaluation:
        doesn’t execute tasks immediately
        maintains chain of operations as meta-data of the job called DAG
        execution happens only when "action" is called on transformations DAG
    concise and consistent APIs in Scala, Java and Python
    written in Scala; runs in JVM
    can process data stored by HBase structure
    can run without Hadoop with apache Mesos or also in standalone mode
    In addition to MR, supports:
        SQL like queries, 
        streaming data, 
        machine learning 
        data processing in terms of graph

Execution flow:
  SparkContext process (a handle to the distributed system)
  Application program (Driver program) uses SparkContext oject
  SparkContext can use several cluster managers for resource allocation
      Spark’s own standalone cluster manager
      Apache Mesos 
      Apache Hadoop's YARN
  Executors: processes on nodes that run computations & store data for app
  Application code is sent to the executors through SparkContext
  Tasks are sent to the executors to run and complete it

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


Apache Spark Architecture Explained in Detail:
==============================================
https://www.dezyre.com/article/apache-spark-architecture-explained-in-detail/338

Two main abstractions:
    Resilient Distributed Datasets (RDD)
    Directed Acyclic Graph (DAG)
    
RDDs:
    -collection of data items split into partitions and can be stored in-memory on workers nodes
    -a distributed memory abstraction that lets programmers perform in-memory computations in a fault-tolerant manner
    -an immutable, partitioned collection of elements that can be operated on in parallel
    
    
                              Create RDD--->RDD--->Transformations
                                    ----    RDD<-----------|
                                  /         RDD<-----------|
                               Lineage      RDD<-----------|
                                  \         RDD<-----------|
                                    ----    RDD<-----------+
                                             |
                                             |  
                                          Actions
                                             |
                                          Results   
    
    Longform:
        RESILIENT
          -provides fault tolerance through lineage graph 
          -which keeps track of transformations to be executed after an action has been called
          -which helps recompute any missing or damaged partitions because of node failures
        Distributed
          -meaning the data is present on multiple nodes in a cluster
        Datasets
          -Collection of partitioned data with primitive values.


        


RDDs can be created by following 3 ways: 
    1. Using Hadoop Datasets already stored on HDFS, HBase, Cassandra, S3, etc
            scala> val data = sc.textFile("data.txt", 4)  #4 is no. of partitions
    2. Using Parallelized collections which are based on existing Scala collections
            scala> val no = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
            scala> val noData = sc.parallelize(no)
    3. Create new RDD from the existing one
            scala> val newRDD = noData.map(X => (X * 2))


RDD’s support two different types of operations – 
    Transformations
    Actions
    
Master/slave architecture with
    A cluster manager
    Two main daemons 
        Master Daemon – (Master/Driver Process) - one
        Worker Daemon – (Slave Process)         - many

Role of Master/Driver in Spark Architecture:
    -central and the entry point of the Spark Shell (Scala, Python, and R)
    -runs the main () function of the application 
    -the place where the Spark Context is created. 
    -contains various components – 
        DAGScheduler, 
        TaskScheduler, 
        BackendScheduler 
        BlockManager
    -schedules the job execution and negotiates with the cluster manager
    -translates the RDD’s into the execution graph and splits the graph into multiple stages
    -stores the metadata about all the RDDs and their partitions
    -its the Cockpits of Jobs and Tasks Execution 
    -converts a user application into smaller execution units known as tasks (which are executed by the executors
    -exposes the information about the running spark application through a Web UI at port 4040

Role of Executor in Spark Architecture:
    -distributed agent responsible for the execution of tasks
    -run for the entire lifetime of a Spark application (“Static Allocation of Executors”)
    -users can also opt for dynamic allocations of executors 
      wherein they can add or remove spark executors dynamically to match with the overall workload.
    -performs all the data processing
    -reads from and writes data to external sources
    -stores the computation results data in-memory, cache or disk

Role of Cluster Manager in Spark Architecture:
    -external service responsible for acquiring resources on the spark cluster and allocating them to a spark job
    -there are 4 different types of cluster managers
    -used for the allocation and deallocation of resources such as memory for client spark jobs, CPU memory, etc. 
        1. Standalone
            -If only Spark is running, then this is one of the easiest to setup cluster manager used for novel deployments
            -In this mode - Spark manages its own cluster
            -Each application runs an executor on every node within the cluster
        2. Hadoop YARN
            -comes with most of the Hadoop distributions and is the only cluster manager in Spark that supports security
            -allows dynamic sharing and central configuration of the same pool of cluster resources between various frameworks
            -number of executors to use can be selected by the user unlike the Standalone mode
        3. Apache Mesos
            -dedicated cluster and resource manager that provides rich resource scheduling capabilities
            -has fine grained sharing option so Spark shell scales down its CPU alloc during execution of multiple commands
        4. Kubernetes
    -the standalone cluster manager is the easiest one to use when developing a new spark application.
    
Run Time Architecture of a Spark Application (What happens when a Spark Job is submitted?):
    ->client submits a spark user application code
      ->driver implicitly converts the code containing transformations and actions into logical DAG
      ->driver also performs certain optimizations like pipelining transformations
        ->converts the logical DAG into physical execution plan with set of stages
          ->converts physical execution plan into small physical execution units i.e. tasks under each stage
            ->then tasks are bundled to be sent to the Spark Cluster
              ->driver talks to cluster manager and negotiates for resources
                ->cluster manager launches executors on the worker nodes on behalf of the driver
                  ->driver sends tasks to the cluster manager based on data placement in cluster
                    ->Before execution, executors register themselves with the driver program
                      ->executors start executing the various tasks assigned by the driver program
                        ->driver monitors the set of executors that are running
                          ->also schedules future tasks based on data placement by tracking the location of cached data
                            ->all executors are terminated and resources are released:
                                -when driver program's main() method exits or 
                                -when it calls the stop () method of the Spark Context


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++



Working with Spark RDD for Fast Data Processing:
================================================
https://www.dezyre.com/article/working-with-spark-rdd-for-fast-data-processing/273

Features of Spark RDDs: (IPL-PF-PT)
-----------------------
    -Immutable: 
        cant be changed once created. 
        But can be transformed into another using transformations like map, filter, join, cogroup, etc. 
        helps attain consistencies in computations
    -Partitioned
        divided into small logical chunks of data - known as partitions
        Partitions are the basic unit of parallelism
        when an action is executed, a task will be launched per partition
        number of partitions are automatically decided; Also, can be specified when creating RDD 
        Partitions of an RDD are distributed through all the nodes in a network
    -Lazy Evaluated
        RDDs are computed in a lazy manner, so that the transformations can be pipelined
        Data in RDDs isn't transformed unless an action that triggers the execution of transformations is invoked
    -Persistence
        makes them good for fast computations
        Users can specify which RDD they want to reuse and select the desired storage for them (on-disk/in-memory)
        RDDs are cacheable i.e. they can hold all the data in desired persistent storage.
    -Fault Tolerance
        RDDs log all transformation in a lineage graph 
          ->whenever a partition is lost, 
            ->lineage graph can be used to replay the transformation 
              ->instead of having to replicate data across multiple nodes (like in Hadoop MapReduce)
    -Parallel
        RDDs in Spark process data in parallel
    -Typed
        Spark RDDs have various types 
          -RDD [int], RDD [long], RDD [string].


Limitations of Hadoop MapReduce that Spark RDDs solve:
------------------------------------------------------
    -example:Making the pizza:
      MR: a piece is cooked by each cook->turn off stove->take piece out->Keep somewhere->Main cook takes it in the end
      Spark: a piece is cooked by each cook->keep stove on->Main cook takes it right away (in-memory)
    -synchronization Barrier between Map and Reduce Tasks
    -Data Sharing is slow with Hadoop MapReduce - Spark RDDs solve this
    -Iterative Operations on Hadoop MapReduce are slow (because of too many disk I/O)
    -Interactive Operations on MapReduce are slow (because of too many disk I/O)

Types of RDDs in Spark:
-----------------------
    MapPartitions RDD : obtained by calling operations like map (), flatMap
    HadoopRDD         : provides functionality for reading data stored in HDFS
    Coalesced RDD     : obtained by calling operations like coalesce and repartition
    Others: SequenceFileRDD, PipedRDD, CoGroupedRDD, ShuffledRDD

RDDs can contain any kind of objects Python, Scala, Java or even user defined class objects


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


Apache Spark Ecosystem and Spark Components:
============================================
https://www.dezyre.com/article/apache-spark-ecosystem-and-spark-components/219

1) Spark Core Component:
------------------------
  Languages Supported by Spark:
      Scala, Python, R, Java
      
  Major Spark Components:
      1. Spark Core
      2. Spark SQL    (for structured data)
      3. Spark Streaming    (for real-time data)
      4. MLlib    (for Machine Learning)
      5. GraphX   (For Graph Processing)
      
  Tachyon:
      In-memory reliable file-system for Spark (Shared Memory in Apache Spark)
      To achieve desired throughput & performance by avoiding unnecessary replications

  Operations on RDDs:
    i) Transformations
        -these are ops like join/union/filter/map on existing RDDs which produce a new RDD, as a result of the operation
        -all transformations in Spark are lazy i.e. 
            Spark does not execute them immediately 
            Instead a lineage is created that tracks all the transformations to be applied on a RDD
    ii) Actions
        -these are ops like count/first/reduce which return values after computations on existing RDDs

2) Spark SQL Component:
-----------------------
  -a library on top of Apache Spark that has been built based on Shark.
    -Shark is open source Hadoop project that uses Apache Spark advanced execution engine to accelerate SQL-like queries
  -can leverage the power of declarative queries and optimized storage by running SQL like queries on Spark datain RDDs
  -can perform extract/transform/load functions on data coming from various formats like JSON/Parquet/Hive 
  -which then can be used to run ad-hoc queries using Spark SQL
  -eases the process of extracting/merging various datasets so that the they are ready to use for machine learning

  -DataFrame constitutes the main abstraction for Spark SQL. 
      Distributed collection of data ordered into named columns => DataFrame in Spark
  -In the earlier versions of Spark SQL, DataFrame == SchemaRDDs
  -Spark's DataFrame API + Spark procedural code => procedural and relational processing capabilities
  -DataFrame API evaluates ops in lazy manner to support relational optimizations & optimizes overall data processing workflow
  -All relational functionalities in Spark can be encapsulated using SparkSQL context or HiveContext

  -Catalyst, an extensible optimizer, is at the core functioning of Spark SQL
  -it is an optimization framework embedded in Scala to help developers improve productivity and perf of the queries
  -Using Catalyst, Spark developers can:
      >briefly specify complex relational optimizations and query transformations in a few lines of code
      >by making the best use of Scala’s powerful programming constructs like pattern matching and runtime meta programming     
  -For machine learning domains, Catalyst eases the process of adding:
      -optimization rules
      -data sources
      -data types


3) Spark Streaming:
-------------------
  -a light weight API that allows developers to perform batch processing and streaming with ease, in the same application
  -"Discretized Streams" form the base abstraction in Spark Streaming (fundamental stream unit)
  -DStream is basically a series of RDDs, to process the real-time data
  -makes use of a continuous stream of input data (Discretized Stream or a series of RDD’s) to process data in real-time
  -leverages fast scheduling capacity of Core to perform streaming analytics by ingesting data in mini-batches
  -Transformations are applied on those mini batches of data
  -Data is ingested from Twitter/Kafka/Akka Actors/IoT Sensors/Kinesis/Flume
  
  Features of Spark Streaming:
    -can reuse the same code for stream and batch processing
    -can also integrate the streaming data with historical data
    -has exactly-once message guarantees and helps recover lost work without having to rewrite any extra code
    -supports inclusion of Spark MLlib for machine learning pipelines into data pathways
  Applications of Spark Streaming
    -used in that applications require real-time statistics and rapid response:
        e.g. alarms, IoT sensors, diagnostics, cyber security, etc.
    -great applications in:
        Log processing, Intrusion Detection and Fraud Detection
    -useful for:
        Online Advertisements and Campaigns, Finance, Supply Chain management, etc.



4) Spark Component MLlib:
-------------------------
  -a low-level machine learning library that can be called from Scala, Python and Java 
  -has implementations for various common machine learning algorithms:
      Clustering- K-means
      Classification – naïve Bayes, logistic regression, SVM
      Decomposition- Principal Component Analysis (PCA) and Singular Value Decomposition (SVD)
      Regression –Linear Regression
      Collaborative Filtering-Alternating Least Squares for Recommendations


5) Spark GraphX:
----------------
  -an API on top of Apache Spark
  -introduces Resilient Distributed Graph (RDG- an abstraction of Spark RDD’s)
  -RDG’s associate records with the vertices and edges in a graph
  -use cases like social network analysis, recommendation and fraud detection





+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Spark Shell Commands to Interact with Spark-Scala:
==================================================
https://data-flair.training/blogs/scala-spark-shell-commands/

Create a new RDD:
    scala> val data = sc.textFile("data.txt")
    OR
    scala> val no = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
    scala> val noData = sc.parallelize(no)
    OR
    scala> val newRDD = noData.map(X => (X * 2))


Get number of Items in the RDD (Action)
    scala> newRDD.count()


Filter Operation  (Transformation)
    scala> val DFData = data.filter(line => line.contains("DataFlair"))


Transformation and Action together
    scala> data.filter(line => line.contains("DataFlair")).count()


Read the first item from the RDD
    scala> data.first()


Read the first 5 item from the RDD
    scala> data.take(5)


Get #RDD Partitions
    scala> data.partitions.length
              Minimum no. of partitions in the RDD is 2 (by default). 
              When we create RDD from HDFS file then a #blocks == #partitions


Cache the file (RDD)
    scala> data.cache()
              RDD will not be cached once you run above operation, check: http://localhost:4040/storage. 
              RDDs will be cached only when we run the Action, which actually needs data read from the disk
    e.g.
    scala> data.count()
    scala> data.collect()


Read Data from HDFS file
    scala> val hFile = sc.textFile("hdfs://localhost:54310/user/hadoop/text.txt")


Spark WordCount Program in Scala
    scala> val wc = hFile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _)
    scala> wc.take(5)


Write the data to HDFS file
    scala> wc.saveAsTextFile("hdfs://localhost:54310/user/hadoop/out.txt")




+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

PySpark Tutorial:
=================
https://www.dezyre.com/apache-spark-tutorial/pyspark-tutorial

./bin/pyspark

>>> RDDread = sc.textFile ("file:////home/hadoop/apache-spark/latest/LICENSE")
>>> RDDread.collect()                       #Get all the content
>>> RDDread.first()                         #only the first line from RDD
>>> RDDread.take(3)                         #only the first 3 lines from RDD returned as python array
>>> RDDread.takeSample(False, 10, 1217)     #Random 10 lines, withReplacement=F, seed as 1217
>>> RDDread.count()                         #Get number of records

#Using External Database
mysql -u root -p
create database demo;
create table demotable(id int,name varchar(20),age int);
use demo;
insert into demotable values(1,"abhay",25),(2,"banti",25);
select * from demotable;

./pyspark --jars /home/hadoop/apache-spark/data/mysql.jar
>>> dataframe_mysql = sqlContext.read.format("jdbc").options( url="jdbc:mysql://:3306/demo",driver = "com.mysql.jdbc.Driver",dbtable = "demotable",user="root", password="root@123").load()
>>> dataframe_mysql.show()

>>> Country = sqlContext.read.format("jdbc").options( url="jdbc:mysql://localhost:3306/world",driver = "com.mysql.jdbc.Driver",dbtable = "country",user="root", password="root@123").load()
>>> Country.persist()
>>> CountryLanguage = sqlContext.read.format("jdbc").options( url="jdbc:mysql://localhost:3306/world",driver = "com.mysql.jdbc.Driver",dbtable = "countrylanguage",user="root", password="root@123").load().persist()
>>> Country.columns
>>> CountryLanguage.columns

>>> country_name = Country.rdd.map(lambda row:(row[0],row[1]))
>>> country_lang = CountryLanguage.rdd.map(lambda row : (row[0],row[1]))
>>> Countryname_language = country_name.join(country_lang)
>>> Countryname_language.take(10)
>>> Countryname_language.distinct().count()   #984


#Transformation and Actions in Apache Spark:
#===========================================
Transformations:
    map()   flatMap()   filter()    sample()    union()   intersection()    distinct()    join()
Actions:
    reduce()  collect()   count()   first()   takeSample()

map() vs. flatMap():
--------------------
      >>> confusedRDD = sc.textFile("../../data/confusion.txt")
      >>> confusedRDD.take(5)
      >>> mappedconfusion = confusedRDD.map(lambda line : line.split(" "))
      >>> mappedconfusion.take(5)
              #Gives array of 5 arrays; each array containing line split by space 
      >>> flatMappedConfusion = confusedRDD.flatMap(lambda line : line.split(" "))
      >>> flatMappedConfusion.take(5)
              #Gives an array of 5 elements i.e. first 5 words of the file

filter():
---------
      >>> onlyconfusion = confusedRDD.filter(lambda line : ("confusion" in line.lower()))
      >>> onlyconfusion.count()
      >>> ankchanges = changesRDD.filter(lambda line : "ankurdave@gmail.com" in line)

sample(withReplacement, fraction, seed):
----------------------------------------
      >>> sampledconfusion = confusedRDD.sample(True,0.5,3) 

union():
--------
      >>> abhay_marks = [("physics",85),("maths",75),("chemistry",95)]
      >>> ankur_marks = [("physics",65),("maths",45),("chemistry",85)]
      >>> abhay = sc.parallelize(abhay_marks)
      >>> ankur = sc.parallelize(ankur_marks)
      >>> abhay.union(ankur).collect()

join():
-------
      >>> Subject_wise_marks = abhay.join(ankur)
      >>> Subject_wise_marks.collect()

intersection():
---------------
      >>> Cricket_team = ["sachin","abhay","michael","rahane","david","ross","raj","rahul","hussy","steven","sourav"]
      >>> Toppers = ["rahul","abhay","laxman","bill","steve"]
      >>> cricketRDD = sc.parallelize(Cricket_team)
      >>> toppersRDD = sc.parallelize(Toppers)
      >>> toppercricketers = cricketRDD.intersection(toppersRDD)
      >>> toppercricketers.collect()

distinct():
-----------
      >>> best_story = ["movie1","movie3","movie7","movie5","movie8"]
      >>> best_direction = ["movie11","movie1","movie5","movie10","movie7"]
      >>> best_screenplay = ["movie10","movie4","movie6","movie7","movie3"]
      >>> story_rdd = sc.parallelize(best_story)
      >>> direction_rdd = sc.parallelize(best_direction)
      >>> screen_rdd = sc.parallelize(best_screenplay)
      >>> total_nomination_rdd = story_rdd.union(direction_rdd).union(screen_rdd)
      >>> total_nomination_rdd.collect()
      >>> unique_movies_rdd = total_nomination_rdd.distinct()
      >>> unique_movies_rdd .collect()
          #['movie10', 'movie3', 'movie11', 'movie1', 'movie6', 'movie7', 'movie4', 'movie5', 'movie8']

RDD Partitions:
+++++++++++++++
    --degree of parallelism or #partitions can be specified:
          >when creating a RDD or 
          >later on using repartition() / coalesce() methods
    --coalesce() is an optimized version of repartition() method 
          >avoids data movement and generally used to decrease #partitions after filtering a large dataset
    --check current #partitions using - rdd.getNumPartitions()
    --two versions of the map which work on each partition of RDD separately
          >partRDD.mapPartitions() : 
            This runs a map operation individually on each partition; not on each line of the entire RDD
          >mapPartitionsWithIndex(n) : 
            Same as above but we can additionally specify the partition number

Caching, Accumulators and UDF:
++++++++++++++++++++++++++++++
Caching:
    decreases the computation time by almost 100X
Accumulators:
    --the global variable that can be shared across tasks
    --write only variables which can be updated by each task
    --aggregated result is propagated to the driver program
UDF (User Defined Functions):
    --simple way to add separate functions into Spark that can be used during various transformation stages

#e.g. of CACHING:
>>> userRDD = sc.textFile("/home/hadoop/apache-spark/data/ml-100k/u.user")
#############     user id | age | gender | occupation | zip code  #############

>>> userRDD.count()     #943
#e.g. of UDFs:
                  def parse_N_calculate_age(data):
                    userid,age,gender,occupation,zip = data.split("|")
                    return  userid, age_group(int(age)),gender,occupation,zip,int(age)
                  def age_group(age):
                    if age < 10 :
                      return '0-10'
                    elif age < 20:
                      return '10-20'
                    elif age < 30:
                      return '20-30'
                    elif age < 40:
                      return '30-40'
                    elif age < 50:
                      return '40-50'
                    elif age < 60:
                      return '50-60'
                    elif age < 70:
                      return '60-70'
                    elif age < 80:
                      return '70-80'
                    else :
                      return '80+'
>>> data_with_age_bucket = userRDD.map(parse_N_calculate_age)
#age group “20-30” for further analysis
>>> RDD_20_30 = data_with_age_bucket.filter(lambda line : '20-30' in line)
#count the number users by their profession in the given age_group 20-30
>>> freq = RDD_20_30.map(lambda line : line[3]).countByValue()
>>> dict(freq)
#count the number of movie users in the same age group based on gender
>>> age_wise = RDD_20_30.map (lambda line : line[2]).countByValue()
>>> dict(age_wise)
#If we are done with operations on RDD_20_30, we can remove it from memory as:
>>> RDD_20_30.unpersist()


#e.g. of ACCUMULATORS:
#use Accumulators for outlier detection. Assume that 
    #anyone who falls into age group 80+ is outlier -> over_age 
    #anyone falling into age group 0-10 is outlier  -> under_age
>>> Under_age = sc.accumulator(0)
>>> Over_age = sc.accumulator(0)
>>> def outliers(data):
      global Over_age, Under_age
      age_grp = data[1]
      if(age_grp == "70-80"):
          Over_age +=1
      if(age_grp == "0-10"):
          Under_age +=1
      return data
>>> df = data_with_age_bucket.map(outliers).collect()
>>> Under_age.value
1
>>> Over_age.value
4


## Running a Spark application in Standalone Mode:
--------------------------------------------------
> Here’s how we can run our previous example in Spark Standalone Mode - (PySparkDemo.py contains all above commands)
hadoop@jarvis:~/apache-spark/latest/bin$ ./spark-submit /home/pradeep/Documents/TESTING/Apoxi/Runtime/scripts/PySparkDemo.py 


## Launching Spark Application on a Cluster
> Start spark daemons (start-all.sh)
> Create the log analysis python program to be run with spark
> Locate the log file to be processed
> Start the spark job:
    spark-submit --master spark://192.168.1.65:7077 --deploy-mode client python_log_read.py



+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Spark Tutorial:
===============

Step 1: Verify if Java is installed
    $java -version

Step 2: Verify if Spark is installed
    $scala -version

Step 3: Download and Install Apache Spark:
    $spark-shell
























